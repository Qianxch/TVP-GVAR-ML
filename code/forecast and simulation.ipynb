{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf07016-ba63-4957-aa0c-b844f410831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import MultiTaskElasticNetCV, RidgeCV, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.api import VAR\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42); random.seed(42)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "plt.rcParams.update({\"font.family\": \"Times New Roman\", \"axes.unicode_minus\": False, \"axes.labelsize\": 13, \"axes.titlesize\": 14,\n",
    "                     \"xtick.labelsize\": 12, \"ytick.labelsize\": 12, \"legend.fontsize\": 11, \"figure.titlesize\": 15,\n",
    "                     \"savefig.dpi\": 300, \"savefig.bbox\": \"tight\", \"pdf.fonttype\": 42, \"ps.fonttype\": 42})\n",
    "\n",
    "# === Trade weights ===\n",
    "def make_trade_weights() -> np.ndarray:\n",
    "    W = np.array([[0.0, 0.137, 0.15], [0.45, 0.0, 0.85], [0.55, 0.863, 0.0]], dtype=float)\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    col = W.sum(axis=0, keepdims=True); col[col == 0] = 1.0\n",
    "    return W / col\n",
    "\n",
    "W_E = make_trade_weights()\n",
    "COUNTRIES = [\"JP\", \"US\", \"EU\"]\n",
    "COLIDX = {c: i for i, c in enumerate(COUNTRIES)}\n",
    "\n",
    "def foreign_weighted_series(df: pd.DataFrame, importer: str, var_suffix: str) -> pd.Series:\n",
    "    j = COLIDX[importer]\n",
    "    out = 0.0\n",
    "    for origin, i in COLIDX.items():\n",
    "        w = W_E[i, j]\n",
    "        if w != 0: out = out + w * df[f\"{origin}_{var_suffix}\"]\n",
    "    return out\n",
    "\n",
    "PRETTY_MAP = {\"JP_GDP_D\": \"Japan ΔGDP\", \"US_GDP_D\": \"US ΔGDP\", \"EU_GDP_D\": \"EU ΔGDP\",\n",
    "              \"JP_CPI_D\": \"Japan ΔCPI\", \"US_CPI_D\": \"US ΔCPI\", \"EU_CPI_D\": \"EU ΔCPI\",\n",
    "              \"JP_HUR_D\": \"Japan ΔUnemp.\", \"US_HUR_D\": \"US ΔUnemp.\", \"EU_HUR_D\": \"EU ΔUnemp.\", \"WTI_D\": \"ΔWTI\"}\n",
    "\n",
    "def pretty(var: str) -> str:\n",
    "    return PRETTY_MAP.get(var, var.replace(\"_\", \" \"))\n",
    "\n",
    "# === Hyper-parameters ===\n",
    "SPLIT_DATE, P_LAG, TEST_HORIZON, ROLL_WIN, LOOK_BACK = \"2022-07-01\", 2, 12, 20, 10\n",
    "USE_SIM = True  # True: use simulated data; False: use real data\n",
    "SIM_T, SIM_START, USE_DEEP = 360, \"2001-01-01\", True\n",
    "\n",
    "BATCH_SIZE, MAX_EPOCH, PATIENCE = 64, 300, 40\n",
    "LSTM_DIMS, GRU_DIMS = (128, 64), (128, 64)\n",
    "DROPOUT_RATE, REC_DROPOUT = 0.3, 0.15\n",
    "RIDGE_ALPHAS = np.logspace(-1, 2, 10)\n",
    "\n",
    "RF_N_ESTIMATORS, RF_MAX_DEPTH, RF_MAX_FEATURES, RF_MIN_SAMPLES_LEAF = 300, 8, \"sqrt\", 1\n",
    "CART_MAX_DEPTH, CART_MIN_SAMPLES_LEAF = 8, 5\n",
    "VAR_P = 1\n",
    "\n",
    "FILE = \"../data/macro_panel.xlsx\"\n",
    "\n",
    "def winsorize(df: pd.DataFrame, k: float = 2.5) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for col in out.columns:\n",
    "        s = out[col]; mu, sd = s.mean(), s.std()\n",
    "        out[col] = s.clip(mu - k * sd, mu + k * sd)\n",
    "    return out\n",
    "\n",
    "def load_data(path: str, P: int):\n",
    "    cols = [\"Date\", \"JP_GDP\", \"US_GDP\", \"EU_GDP\", \"JP_CPI\", \"US_CPI\", \"EU_CPI\", \"JP_HUR\", \"US_HUR\", \"EU_HUR\", \"WTI\"]\n",
    "    df = pd.read_excel(path, usecols=cols).assign(Date=lambda x: pd.to_datetime(x[\"Date\"])).set_index(\"Date\").sort_index()\n",
    "    df = winsorize(df)\n",
    "\n",
    "    df[\"WTI_D\"] = df[\"WTI\"].diff(); df.drop(columns=\"WTI\", inplace=True)\n",
    "    for v in (\"GDP\", \"CPI\", \"HUR\"):\n",
    "        for c in (\"JP\", \"US\", \"EU\"):\n",
    "            df[f\"{c}_{v}_D\"] = df[f\"{c}_{v}\"].diff(); df.drop(columns=f\"{c}_{v}\", inplace=True)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    for c in (\"JP\", \"US\", \"EU\"):\n",
    "        for v in (\"GDP_D\", \"CPI_D\", \"HUR_D\"):\n",
    "            df[f\"{c}*FOR*{v}\"] = foreign_weighted_series(df, importer=c, var_suffix=v)\n",
    "\n",
    "    data = df.copy()\n",
    "    for lag in range(1, P + 1):\n",
    "        data = data.join(data.shift(lag).add_suffix(f\"_lag{lag}\"))\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    split_ts = pd.Timestamp(SPLIT_DATE)\n",
    "    tmp = data[data.index > split_ts].head(P_LAG + TEST_HORIZON)\n",
    "    warm, test, train = tmp.iloc[:P_LAG], tmp.iloc[P_LAG:], data[data.index <= split_ts]\n",
    "    return train, warm, test\n",
    "\n",
    "# === Simulation branch ===\n",
    "if USE_SIM:\n",
    "    import sim_tvp_5gvar as sim\n",
    "    T = SIM_T\n",
    "\n",
    "    def months_between(a, b): return (b.year - a.year) * 12 + (b.month - a.month)\n",
    "\n",
    "    START_TS, SPLIT_TS = pd.Timestamp(SIM_START), pd.Timestamp(SPLIT_DATE)\n",
    "    BREAK_AT = months_between(START_TS, SPLIT_TS) + P_LAG\n",
    "    BREAK_AT = max(P_LAG + 1, min(T - 2, BREAK_AT))\n",
    "\n",
    "    SCENARIO_TAG = \"A\"  # \"A\"/\"B\"/\"C\"/\"D\"\n",
    "\n",
    "    if SCENARIO_TAG == \"A\":\n",
    "        params = dict(tvp=True, q_scale=0.015, eps_scale=0.020, hetero=False, break_at=BREAK_AT, trend_strength=0.0003, boost_mult=1.4, boost_block=\"FOREIGN\")\n",
    "    elif SCENARIO_TAG == \"B\":\n",
    "        params = dict(tvp=False, q_scale=0.0, eps_scale=0.070, hetero=True, break_at=BREAK_AT, trend_strength=0.0, boost_mult=1.0, boost_block=None)\n",
    "    elif SCENARIO_TAG == \"C\":\n",
    "        params = dict(tvp=True, q_scale=0.020, eps_scale=0.055, hetero=True, break_at=int(0.35 * T), trend_strength=0.001, boost_mult=2.3, boost_block=\"FOREIGN\")\n",
    "    elif SCENARIO_TAG == \"D\":\n",
    "        params = dict(tvp=True, q_scale=0.001, eps_scale=0.020, hetero=True, break_at=BREAK_AT, trend_strength=0.0, boost_mult=2.0, boost_block=\"FOREIGN\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scenario tag.\")\n",
    "\n",
    "    betas = sim.gen_beta_paths(T=T, P=P_LAG, tvp=params[\"tvp\"], break_at=params[\"break_at\"], q_scale=params[\"q_scale\"],\n",
    "                              trend_strength=params[\"trend_strength\"], boost_mult=params[\"boost_mult\"], boost_block=params[\"boost_block\"])\n",
    "    df_levels = sim.simulate_panel(T=T, P=P_LAG, betas=betas, start_date=SIM_START, hetero=params[\"hetero\"],\n",
    "                                   break_at=params[\"break_at\"], eps_scale=params[\"eps_scale\"])\n",
    "\n",
    "    if SCENARIO_TAG in {\"B\", \"D\"}:\n",
    "        spike_idx = min(params[\"break_at\"], T - 3)\n",
    "        spike_date, next_date = df_levels.index[spike_idx], df_levels.index[spike_idx + 1]\n",
    "        shock_cols = [c for c in df_levels.columns if any(key in c for key in [\"GDP\", \"CPI\", \"HUR\"])]\n",
    "        scale = 4.0; jump = scale * df_levels[shock_cols].std()\n",
    "        df_levels.loc[spike_date, shock_cols] += jump; df_levels.loc[next_date, shock_cols] -= jump\n",
    "        print(f\"[Scenario {SCENARIO_TAG}] Spike at {spike_date.date()} for {len(shock_cols)} series (scale={scale}x std).\")\n",
    "\n",
    "    data = sim.add_foreign_and_lags(df_levels, P_LAG)\n",
    "    split_ts = pd.Timestamp(SPLIT_DATE)\n",
    "    tmp = data[data.index > split_ts].head(P_LAG + TEST_HORIZON)\n",
    "    warm_df, test_df, train_df = tmp.iloc[:P_LAG], tmp.iloc[P_LAG:], data[data.index <= split_ts]\n",
    "    assert len(test_df) == TEST_HORIZON\n",
    "else:\n",
    "    SCENARIO_TAG = \"REAL DATA\"\n",
    "    train_df, warm_df, test_df = load_data(FILE, P_LAG)\n",
    "\n",
    "H = len(test_df); assert H == TEST_HORIZON\n",
    "\n",
    "# === Saving ===\n",
    "SAVE_DIR = \"../results\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def save_pdf_if_real(fig, filename: str):\n",
    "    if not USE_SIM:\n",
    "        path = os.path.join(SAVE_DIR, filename); fig.savefig(path, format=\"pdf\"); print(f\"[Saved] {path}\")\n",
    "\n",
    "eq_list = [f\"{c}_{v}\" for c in (\"JP\", \"US\", \"EU\") for v in (\"GDP_D\", \"CPI_D\", \"HUR_D\")] + [\"WTI_D\"]\n",
    "\n",
    "reg_map: Dict[str, list] = {}\n",
    "int_idx: list[int] = []\n",
    "eq_size: Dict[str, int] = {}\n",
    "cur = 0\n",
    "\n",
    "for eq in eq_list:\n",
    "    if eq == \"WTI_D\":\n",
    "        feats = [f\"WTI_D_lag{lag}\" for lag in range(1, P_LAG + 1)]\n",
    "    else:\n",
    "        c, v = eq.split(\"_\", 1)\n",
    "        own = [f\"{c}_{v}_lag{lag}\" for lag in range(1, P_LAG + 1)]\n",
    "        forw = [f\"{c}*FOR*{v}_lag{lag}\" for lag in range(1, P_LAG + 1)]\n",
    "        wti = [f\"WTI_D_lag{lag}\" for lag in range(1, P_LAG + 1)]\n",
    "        feats = own + forw + wti\n",
    "    reg_map[eq] = feats\n",
    "    k = 1 + len(feats)\n",
    "    int_idx.append(cur); eq_size[eq] = k; cur += k\n",
    "\n",
    "PARAM_LEN = cur\n",
    "\n",
    "# === Beta estimation ===\n",
    "def beta_ols(df_blk: pd.DataFrame) -> np.ndarray:\n",
    "    def fit1(X, y):\n",
    "        try: return HuberRegressor().fit(X, y)\n",
    "        except Exception: return RidgeCV(alphas=RIDGE_ALPHAS).fit(X, y)\n",
    "\n",
    "    b = np.zeros(PARAM_LEN)\n",
    "    for eq, idx in zip(eq_list, int_idx):\n",
    "        X, y = df_blk[reg_map[eq]].values, df_blk[eq].values\n",
    "        mdl = fit1(X, y)\n",
    "        b[idx] = mdl.intercept_\n",
    "        b[idx + 1: idx + 1 + len(mdl.coef_)] = mdl.coef_\n",
    "    return b\n",
    "\n",
    "def kf_q0_auto(train_df: pd.DataFrame, roll_win: int = 20, floor: float = 5e-5, scale: float = 0.6) -> float:\n",
    "    Bs = []\n",
    "    for i in range(roll_win - 1, len(train_df)):\n",
    "        blk = train_df.iloc[i - roll_win + 1: i + 1]\n",
    "        Bs.append(beta_ols(blk))\n",
    "    B = np.vstack(Bs)\n",
    "    dB = np.diff(B, axis=0)\n",
    "    q0 = float(np.median(np.var(dB, axis=0))) * scale\n",
    "    return max(q0, floor)\n",
    "\n",
    "def beta_kf(df_blk: pd.DataFrame, q0: float = 2e-4, r_scale: float = 1.0) -> np.ndarray:\n",
    "    b = np.zeros(PARAM_LEN)\n",
    "    for eq, idx in zip(eq_list, int_idx):\n",
    "        X = df_blk[reg_map[eq]].values\n",
    "        y = df_blk[eq].values.reshape(-1, 1)\n",
    "        n, p = X.shape; m = p + 1\n",
    "        beta = np.zeros((m, 1)); P = 1e4 * np.eye(m)\n",
    "        X1 = np.c_[np.ones(n), X]\n",
    "        beta_ols_local = np.linalg.lstsq(X1, y, rcond=None)[0]\n",
    "        y_hat_ols = X1 @ beta_ols_local\n",
    "        r = float(np.var(y - y_hat_ols, ddof=min(5, n - 1))) * r_scale + 1e-8\n",
    "        q = q0 * np.ones(m); I = np.eye(m)\n",
    "\n",
    "        for t in range(n):\n",
    "            Ht = np.r_[ [1.0], X[t] ].reshape(1, -1)\n",
    "            P = P + np.diag(q)\n",
    "            S = Ht @ P @ Ht.T + r\n",
    "            K = (P @ Ht.T) / float(S)\n",
    "            e = y[t: t + 1] - Ht @ beta\n",
    "            beta = beta + K * e\n",
    "            P = (I - K @ Ht) @ P\n",
    "\n",
    "        b[idx: idx + m] = beta.flatten()\n",
    "    return b\n",
    "\n",
    "KF_Q0 = kf_q0_auto(train_df, roll_win=ROLL_WIN, floor=5e-5, scale=0.6)\n",
    "block_beta = lambda df_blk: beta_kf(df_blk, q0=KF_Q0, r_scale=1.0)\n",
    "\n",
    "beta_series = np.vstack([block_beta(train_df.iloc[i - ROLL_WIN + 1: i + 1]) for i in range(ROLL_WIN - 1, len(train_df))])\n",
    "beta_static = block_beta(train_df)\n",
    "\n",
    "# === Residual scale (EWMA) ===\n",
    "sigma = {eq: 0.0 for eq in eq_list}\n",
    "lam = 0.95\n",
    "\n",
    "for eq, idx in zip(eq_list, int_idx):\n",
    "    X = train_df[reg_map[eq]]\n",
    "    y = train_df[eq]\n",
    "    beta_vec = beta_static[idx: idx + eq_size[eq]]\n",
    "    eps = y - (beta_vec[0] + X.values.dot(beta_vec[1:]))\n",
    "    s = 0.0\n",
    "    for e in eps: s = lam * s + (1 - lam) * abs(e)\n",
    "    sigma[eq] = max(s, 1e-6)\n",
    "\n",
    "# === Deep model data (beta paths) ===\n",
    "scaler_beta = StandardScaler().fit(beta_series)\n",
    "beta_scaled = scaler_beta.transform(beta_series)\n",
    "\n",
    "N_PCA = beta_scaled.shape[1]\n",
    "pca = PCA(n_components=N_PCA)\n",
    "beta_low = pca.fit_transform(beta_scaled)\n",
    "\n",
    "X_dl, y_dl = [], []\n",
    "for i in range(len(beta_low) - LOOK_BACK):\n",
    "    X_dl.append(beta_low[i: i + LOOK_BACK]); y_dl.append(beta_low[i + LOOK_BACK])\n",
    "X_dl, y_dl = np.array(X_dl), np.array(y_dl)\n",
    "\n",
    "# === Beta forecasters (8 models) ===\n",
    "beta_hat: Dict[str, np.ndarray] = {}\n",
    "beta_hat[\"CONSTANT\"] = np.tile(beta_series[-ROLL_WIN:].mean(0), (H, 1))\n",
    "beta_hat[\"VAR\"] = VAR(beta_series).fit(maxlags=VAR_P).forecast(beta_series[-VAR_P:], H)\n",
    "\n",
    "def cart_fc_delta() -> np.ndarray:\n",
    "    Y = beta_series[1:] - beta_series[:-1]; X = beta_series[:-1]\n",
    "    m = DecisionTreeRegressor(max_depth=CART_MAX_DEPTH, min_samples_leaf=CART_MIN_SAMPLES_LEAF, random_state=42).fit(X, Y)\n",
    "    prev = beta_series[-1].copy(); ps = []\n",
    "    for _ in range(H):\n",
    "        d = m.predict(prev[None, :])[0]; prev = prev + d; ps.append(prev.copy())\n",
    "    return np.vstack(ps)\n",
    "\n",
    "beta_hat[\"CART\"] = cart_fc_delta()\n",
    "\n",
    "def rf_fc_delta() -> np.ndarray:\n",
    "    Y = beta_series[1:] - beta_series[:-1]; X = beta_series[:-1]\n",
    "    m = RandomForestRegressor(n_estimators=RF_N_ESTIMATORS, max_depth=RF_MAX_DEPTH, max_features=RF_MAX_FEATURES,\n",
    "                             min_samples_leaf=RF_MIN_SAMPLES_LEAF, n_jobs=-1, random_state=42).fit(X, Y)\n",
    "    prev = beta_series[-1].copy(); ps = []\n",
    "    for _ in range(H):\n",
    "        d = m.predict(prev[None, :])[0]; prev = prev + d; ps.append(prev.copy())\n",
    "    return np.vstack(ps)\n",
    "\n",
    "beta_hat[\"RF\"] = rf_fc_delta()\n",
    "\n",
    "def lasso_fc_delta(beta_series: np.ndarray, alpha_grid=np.logspace(-4, 0, 30), decay: float = 0.985, l1_ratio: float = 1.0) -> np.ndarray:\n",
    "    dB = np.diff(beta_series, axis=0); B = beta_series[:-1]\n",
    "    Z = np.hstack([B, dB]); Y = dB\n",
    "\n",
    "    X_scaler = StandardScaler().fit(Z); Y_scaler = StandardScaler().fit(Y)\n",
    "    Xs = X_scaler.transform(Z); Ys = Y_scaler.transform(Y)\n",
    "\n",
    "    n = len(Xs)\n",
    "    w = np.array([decay ** (n - 1 - i) for i in range(n)])\n",
    "    sw = np.sqrt(w)[:, None]\n",
    "    Xsw, Ysw = Xs * sw, Ys * sw\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    lass = MultiTaskElasticNetCV(l1_ratio=l1_ratio, alphas=alpha_grid, cv=tscv, fit_intercept=True, n_jobs=-1, random_state=42)\n",
    "    lass.fit(Xsw, Ysw)\n",
    "\n",
    "    support = np.any(lass.coef_ != 0, axis=0)\n",
    "\n",
    "    if support.sum() >= 1:\n",
    "        Xs_sub = Xs[:, support]; Xsw_sub = Xs_sub * sw\n",
    "        ridge = RidgeCV(alphas=RIDGE_ALPHAS).fit(Xsw_sub, Ysw)\n",
    "        full_coef = np.zeros((PARAM_LEN, Xs.shape[1])); full_coef[:, support] = ridge.coef_\n",
    "        ridge_intercept = ridge.intercept_\n",
    "\n",
    "        def step(prev_beta, prev_dB):\n",
    "            z = np.hstack([prev_beta, prev_dB]).reshape(1, -1)\n",
    "            z_s = X_scaler.transform(z)\n",
    "            dB_s = z_s @ full_coef.T + ridge_intercept\n",
    "            return Y_scaler.inverse_transform(dB_s).ravel()\n",
    "    else:\n",
    "        def step(prev_beta, prev_dB): return np.zeros_like(prev_beta)\n",
    "\n",
    "    prev = beta_series[-1].copy(); prev_d = np.zeros_like(prev); out = []\n",
    "    for _ in range(H):\n",
    "        d = step(prev, prev_d); prev = prev + d; prev_d = d; out.append(prev.copy())\n",
    "    return np.vstack(out)\n",
    "\n",
    "beta_hat[\"LASSO\"] = lasso_fc_delta(beta_series)\n",
    "\n",
    "def rnn_fc(cell, dims: tuple) -> np.ndarray:\n",
    "    \"\"\"RNN forecaster for beta_t.\"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    inp = Input(shape=(LOOK_BACK, N_PCA))\n",
    "    x = inp\n",
    "    for h in dims:\n",
    "        x = cell(h, dropout=DROPOUT_RATE, recurrent_dropout=REC_DROPOUT, return_sequences=True)(x)\n",
    "    x = cell(dims[-1] // 2, dropout=DROPOUT_RATE, recurrent_dropout=REC_DROPOUT)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    out = Dense(N_PCA)(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    ck = ModelCheckpoint(\"best_tmp.keras\", save_best_only=True, monitor=\"val_loss\", mode=\"min\", verbose=0)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_dl, y_dl, epochs=MAX_EPOCH, batch_size=BATCH_SIZE, validation_split=0.2, verbose=0, callbacks=[ck, es])\n",
    "\n",
    "    seq_low = beta_low[-LOOK_BACK:]; preds_low = []\n",
    "    for _ in range(H):\n",
    "        nxt_low = model.predict(seq_low[None, :, :], verbose=0).squeeze()\n",
    "        preds_low.append(nxt_low.copy())\n",
    "        seq_low = np.vstack([seq_low[1:], nxt_low])\n",
    "\n",
    "    preds_low = np.vstack(preds_low)\n",
    "    beta_scaled_hat = pca.inverse_transform(preds_low)\n",
    "    beta_hat_full = scaler_beta.inverse_transform(beta_scaled_hat)\n",
    "    return beta_hat_full\n",
    "\n",
    "USE_DEEP = USE_DEEP and (len(beta_scaled) >= 50)\n",
    "if USE_DEEP:\n",
    "    lstm_pred, gru_pred = rnn_fc(LSTM, LSTM_DIMS), rnn_fc(GRU, GRU_DIMS)\n",
    "    beta_hat[\"LSTM\"], beta_hat[\"GRU\"], beta_hat[\"LSTM-GRU\"] = lstm_pred, gru_pred, 0.5 * lstm_pred + 0.5 * gru_pred\n",
    "\n",
    "# === Soft caps for beta ===\n",
    "def _soft_cap(vec: np.ndarray, lim: float) -> np.ndarray:\n",
    "    return np.tanh(vec / max(lim, 1e-6)) * lim\n",
    "\n",
    "def cap_beta(bt: np.ndarray, P: int = P_LAG, caps=(0.9, 0.45, 0.35), cap_scale: float = 1.0) -> np.ndarray:\n",
    "    own_cap, frm_cap, wti_cap = caps\n",
    "    own_cap *= cap_scale; frm_cap *= cap_scale; wti_cap *= cap_scale\n",
    "\n",
    "    out = bt.copy()\n",
    "    for eq, idx in zip(eq_list, int_idx):\n",
    "        k = eq_size[eq] - 1\n",
    "        coefs = out[idx + 1: idx + 1 + k]\n",
    "        if eq == \"WTI_D\":\n",
    "            out[idx + 1: idx + 1 + k] = _soft_cap(coefs, 0.9 / P)\n",
    "        else:\n",
    "            own = _soft_cap(coefs[:P], 0.9 / P)\n",
    "            frm = _soft_cap(coefs[P: 2 * P], 0.4 / P)\n",
    "            wti = _soft_cap(coefs[2 * P: 3 * P], 0.4 / P)\n",
    "            out[idx + 1: idx + 1 + k] = np.r_[own, frm, wti]\n",
    "    return out\n",
    "\n",
    "# === Out-of-sample beta paths ===\n",
    "def beta_oos(train_df: pd.DataFrame, test_df: pd.DataFrame) -> np.ndarray:\n",
    "    all_df = pd.concat([train_df, test_df])\n",
    "    T_train = len(train_df); out = []\n",
    "    for t in range(T_train, T_train + len(test_df)):\n",
    "        blk = all_df.iloc[t - ROLL_WIN + 1: t + 1]\n",
    "        out.append(block_beta(blk))\n",
    "    return np.vstack(out)\n",
    "\n",
    "beta_true_oos = beta_oos(train_df, test_df)\n",
    "keep_mask = np.ones(PARAM_LEN, dtype=bool)\n",
    "for idx in int_idx: keep_mask[idx] = False\n",
    "\n",
    "# === Macro forecast ===\n",
    "hist0 = [warm_df.iloc[-i - 1].to_dict() for i in range(P_LAG)]\n",
    "\n",
    "def foreign_state(importer: str, var_suffix: str, state_row: dict) -> float:\n",
    "    j = COLIDX[importer]\n",
    "    val = 0.0\n",
    "    for origin, i in COLIDX.items():\n",
    "        w = W_E[i, j]\n",
    "        if w != 0: val += w * state_row[f\"{origin}_{var_suffix}\"]\n",
    "    return val\n",
    "\n",
    "def forecast_macro(beta_seq: np.ndarray, smooth_alpha: float = 0.05, add_noise: bool = False, first_step_beta_blend: bool = True,\n",
    "                   gamma: float = 0.2, cap_scale_t0: float = 0.5, anchor_first_step: bool = True, anchor_hur_lambda: float = 0.8) -> pd.DataFrame:\n",
    "    hist = [h.copy() for h in hist0]\n",
    "    preds = []\n",
    "    beta_prev = beta_series[-1]\n",
    "\n",
    "    for t in range(H):\n",
    "        bt = beta_seq[t]\n",
    "        if t == 0 and first_step_beta_blend:\n",
    "            bt = beta_prev + gamma * (bt - beta_prev)\n",
    "        elif smooth_alpha > 0:\n",
    "            bt = (1 - smooth_alpha) * bt + smooth_alpha * beta_prev\n",
    "\n",
    "        bt = cap_beta(bt, P=P_LAG, cap_scale=(cap_scale_t0 if t == 0 else 1.0))\n",
    "\n",
    "        nxt = {}\n",
    "        for eq, idx in zip(eq_list, int_idx):\n",
    "            if eq == \"WTI_D\":\n",
    "                x = np.array([hist[i - 1][\"WTI_D\"] for i in range(1, P_LAG + 1)])\n",
    "            else:\n",
    "                c, v = eq.split(\"_\", 1)\n",
    "                arr = []\n",
    "                for i in range(1, P_LAG + 1):\n",
    "                    arr += [hist[i - 1][f\"{c}_{v}\"], foreign_state(importer=c, var_suffix=v, state_row=hist[i - 1]), hist[i - 1][\"WTI_D\"]]\n",
    "                x = np.array(arr)\n",
    "\n",
    "            val = bt[idx] + bt[idx + 1: idx + 1 + len(x)].dot(x)\n",
    "            if add_noise: val += 0.35 * np.random.normal(0, sigma[eq])\n",
    "\n",
    "            if anchor_first_step and t == 0 and eq.endswith(\"HUR_D\"):\n",
    "                last_obs = hist[0][eq]; lam_h = anchor_hur_lambda\n",
    "                val = (1.0 - lam_h) * val + lam_h * last_obs\n",
    "\n",
    "            nxt[eq] = val\n",
    "\n",
    "        preds.append(nxt)\n",
    "        hist.insert(0, nxt); hist = hist[:P_LAG]\n",
    "        beta_prev = bt\n",
    "\n",
    "    return pd.DataFrame(preds, index=test_df.index)[eq_list]\n",
    "\n",
    "# === Evaluation & plots ===\n",
    "macro_pred = {k: forecast_macro(v, smooth_alpha=0.05, add_noise=False) for k, v in beta_hat.items()}\n",
    "eq_list_9 = [v for v in eq_list if v != \"WTI_D\"]\n",
    "\n",
    "macro_mse = {k: mean_squared_error(test_df[eq_list], macro_pred[k][eq_list]) for k in macro_pred}\n",
    "macro_mse_9 = {k: mean_squared_error(test_df[eq_list_9], macro_pred[k][eq_list_9]) for k in macro_pred}\n",
    "\n",
    "beta_keys = list(beta_hat.keys())\n",
    "beta_mse = {k: mean_squared_error(beta_true_oos, beta_hat[k]) for k in beta_keys}\n",
    "beta_mse_no_intercept = {k: mean_squared_error(beta_true_oos[:, keep_mask], beta_hat[k][:, keep_mask]) for k in beta_keys}\n",
    "\n",
    "print(\"\\nBeta-MSE (with intercept)\\n\", pd.Series(beta_mse).sort_values().round(4))\n",
    "print(\"\\nMacro-MSE (9 vars, excl. WTI)\\n\", pd.Series(macro_mse_9).sort_values().round(4))\n",
    "\n",
    "mse9_series = pd.Series(macro_mse_9).sort_values()\n",
    "best9 = mse9_series.idxmin()\n",
    "other_models = [m for m in mse9_series.index if m != best9]\n",
    "\n",
    "print(f\"\\n>>> Best model by 9-variable Macro-MSE = {best9}\")\n",
    "print(f\">>> Other models (appendix) = {other_models}\")\n",
    "\n",
    "START = pd.Timestamp(\"2019-01-01\")\n",
    "SPLIT_TS = pd.Timestamp(SPLIT_DATE)\n",
    "ACT_COLOR, FC_COLOR = \"#1f77b4\", \"#ff7f0e\"\n",
    "LW_MAIN, LW_FC = 2.0, 2.0\n",
    "\n",
    "def short_title(var_diff: str) -> str:\n",
    "    if var_diff == \"WTI_D\": return \"ΔWTI\"\n",
    "    country = var_diff.split(\"_\", 1)[0]\n",
    "    if var_diff.endswith(\"GDP_D\"): return f\"{country}(Δ GDP)\"\n",
    "    if var_diff.endswith(\"CPI_D\"): return f\"{country}(ΔCPI)\"\n",
    "    if var_diff.endswith(\"HUR_D\"): return f\"{country}(HUR)\"\n",
    "    return var_diff.replace(\"_\", \" \")\n",
    "\n",
    "def plot_9vars_panel_for_model(model_name: str, save_pdf: bool = False):\n",
    "    prd_model = macro_pred[model_name]\n",
    "    ys = []\n",
    "    for var in eq_list_9:\n",
    "        actual_all = pd.concat([train_df[var], test_df[var]]).loc[START:]\n",
    "        pred_use = prd_model[var].loc[START:]\n",
    "        ys.append(actual_all.values); ys.append(pred_use.values)\n",
    "    y_min, y_max = min(np.min(y) for y in ys), max(np.max(y) for y in ys)\n",
    "\n",
    "    pad = 0.05 * (y_max - y_min + 1e-12)\n",
    "    y_lim = (y_min - pad, y_max + pad)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(16, 10), sharex=False)\n",
    "    for i, var in enumerate(eq_list_9):\n",
    "        ax = axs[i // 3, i % 3]\n",
    "        actual_all = pd.concat([train_df[var], test_df[var]]).loc[START:]\n",
    "        pred_use = prd_model[var].loc[START:]\n",
    "\n",
    "        ax.plot(actual_all.index, actual_all.values, color=ACT_COLOR, lw=LW_MAIN, label=\"Actual\")\n",
    "        ax.plot(pred_use.index, pred_use.values, color=FC_COLOR, lw=LW_FC, ls=\"--\", label=model_name)\n",
    "\n",
    "        ax.axvline(SPLIT_TS, ls=\":\", color=\"gray\", lw=1.0)\n",
    "        ax.set_xlim(START, test_df.index[-1])\n",
    "        ax.set_ylim(*y_lim)\n",
    "        ax.set_title(short_title(var), fontsize=18)\n",
    "\n",
    "        if i == 0: ax.legend(fontsize=10, frameon=False)\n",
    "        else: ax.legend().remove()\n",
    "\n",
    "        ax.grid(axis=\"both\", linestyle=\"--\", alpha=0.25)\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_pdf: save_pdf_if_real(fig, f\"{model_name}_9vars.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Best model: 9 vars panel + WTI ---\n",
    "plot_9vars_panel_for_model(best9, save_pdf=True)\n",
    "\n",
    "fig_wti = plt.figure(figsize=(8, 4))\n",
    "act_wti = pd.concat([train_df[\"WTI_D\"], test_df[\"WTI_D\"]]).loc[START:]\n",
    "prd_wti = macro_pred[best9][\"WTI_D\"].loc[START:]\n",
    "\n",
    "plt.plot(act_wti.index, act_wti.values, color=ACT_COLOR, lw=LW_MAIN, label=\"Actual ΔWTI\")\n",
    "plt.plot(prd_wti.index, prd_wti.values, color=FC_COLOR, lw=LW_FC, ls=\"--\", label=best9)\n",
    "plt.axvline(SPLIT_TS, ls=\":\", color=\"gray\", lw=1.0)\n",
    "plt.xlim(START, test_df.index[-1])\n",
    "plt.legend(frameon=False)\n",
    "plt.grid(ls=\"--\", alpha=0.3)\n",
    "plt.title(\"ΔWTI\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "save_pdf_if_real(fig_wti, f\"{best9}_WTI.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# --- Other models (appendix) ---\n",
    "for mdl in other_models:\n",
    "    print(f\"\\n[Appendix] {mdl}\")\n",
    "    plot_9vars_panel_for_model(mdl, save_pdf=True)\n",
    "\n",
    "# === Single-run wrapper for Monte Carlo ===\n",
    "def forecast_once(train_df: pd.DataFrame, warm_df: pd.DataFrame, test_df: pd.DataFrame, scenario_tag: str = \"REAL DATA\", seed: int = 0):\n",
    "    np.random.seed(seed); random.seed(seed)\n",
    "    macro_pred_local = {k: forecast_macro(v, smooth_alpha=0.0, add_noise=False) for k, v in beta_hat.items()}\n",
    "    eq_list_9_local = [v for v in eq_list if v != \"WTI_D\"]\n",
    "    macro_mse_9_local = {k: mean_squared_error(test_df[eq_list_9_local], macro_pred_local[k][eq_list_9_local]) for k in macro_pred_local}\n",
    "    return macro_mse_9_local\n",
    "\n",
    "# === Monte Carlo (simulation only) ===\n",
    "if USE_SIM:\n",
    "    from sim_tvp_5gvar import simulate_and_preprocess\n",
    "\n",
    "    R = 100\n",
    "    scenario_tag = \"A\"\n",
    "    records = []\n",
    "\n",
    "    for r in range(R):\n",
    "        train_df_mc, test_df_mc, df_levels_mc = simulate_and_preprocess(T=SIM_T, P=P_LAG, split_date=SPLIT_DATE, tvp=params[\"tvp\"],\n",
    "                                                                        break_at=params[\"break_at\"], hetero=params[\"hetero\"], q_scale=params[\"q_scale\"],\n",
    "                                                                        eps_scale=params[\"eps_scale\"], trend_strength=params[\"trend_strength\"])\n",
    "        warm_df_mc = df_levels_mc[df_levels_mc.index > pd.Timestamp(SPLIT_DATE)].head(P_LAG)\n",
    "\n",
    "        res = forecast_once(train_df_mc, warm_df_mc, test_df_mc, scenario_tag, seed=r)\n",
    "        res[\"rep\"] = r\n",
    "        records.append(res)\n",
    "\n",
    "    df_res = pd.DataFrame(records).set_index(\"rep\")\n",
    "    print(f\"\\n=== Scenario {scenario_tag}: Macro-MSE(9 vars) Monte Carlo ===\")\n",
    "    print(df_res.describe().round(4))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    preferred_order = [\"CONSTANT\", \"VAR\", \"CART\", \"RF\", \"LASSO\", \"LSTM\", \"GRU\", \"LSTM-GRU\"]\n",
    "    models = [m for m in preferred_order if m in df_res.columns]\n",
    "    df_res = df_res[models]\n",
    "    data = [df_res[m] for m in models]\n",
    "\n",
    "    box = ax.boxplot(data, showmeans=False, showfliers=False, patch_artist=True,\n",
    "                     boxprops=dict(facecolor=\"#1f77b4\", alpha=0.7, linewidth=1.2),\n",
    "                     whiskerprops=dict(color=\"gray\", linewidth=1.2),\n",
    "                     capprops=dict(color=\"gray\", linewidth=1.2),\n",
    "                     medianprops=dict(color=\"black\", linewidth=1.3))\n",
    "\n",
    "    labels_pretty = [m.replace(\"-\", \"\\n\") if len(m) > 10 else m for m in models]\n",
    "    ax.set_xticks(range(1, len(models) + 1))\n",
    "    ax.set_xticklabels(labels_pretty, fontsize=14, ha=\"center\", linespacing=1.1)\n",
    "    ax.margins(x=0.05)\n",
    "\n",
    "    all_medians = [float(np.median(vals)) for vals in data]\n",
    "    best_median = min(all_medians)\n",
    "\n",
    "    for i, med in enumerate(all_medians):\n",
    "        median_line = box[\"medians\"][i]\n",
    "        y_med = median_line.get_ydata()[0]\n",
    "        ax.text(i + 1, y_med, f\"{med:.4f}\", ha=\"center\", va=\"bottom\", fontsize=15,\n",
    "                weight=\"bold\" if med == best_median else \"normal\", color=\"black\")\n",
    "\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"Macro-MSE\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "    ax.set_facecolor(\"#fafafa\")\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.18, top=0.9)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(SAVE_DIR, f\"MacroMSE_Box_{scenario_tag}.pdf\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[Saved] {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
